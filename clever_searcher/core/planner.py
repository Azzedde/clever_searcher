"""LLM-based planning component for query generation and crawl strategy"""

import json
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta

from openai import OpenAI

from ..utils.config import settings

logger = logging.getLogger(__name__)


@dataclass
class CrawlPlan:
    """Represents a crawl plan generated by the LLM"""
    category: str
    queries: List[str]
    must_have_keywords: List[str]
    avoid_keywords: List[str]
    preferred_sites: List[str]
    avoid_domains: List[str]
    max_pages: int
    max_pages_per_domain: int
    freshness_days: int
    include_news: bool
    time_range: Optional[str]  # 'd', 'w', 'm', 'y'
    priority_score: float
    estimated_duration_minutes: int
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "category": self.category,
            "queries": self.queries,
            "must_have_keywords": self.must_have_keywords,
            "avoid_keywords": self.avoid_keywords,
            "preferred_sites": self.preferred_sites,
            "avoid_domains": self.avoid_domains,
            "max_pages": self.max_pages,
            "max_pages_per_domain": self.max_pages_per_domain,
            "freshness_days": self.freshness_days,
            "include_news": self.include_news,
            "time_range": self.time_range,
            "priority_score": self.priority_score,
            "estimated_duration_minutes": self.estimated_duration_minutes,
        }


class LLMPlanner:
    """LLM-powered planning for web discovery"""
    
    def __init__(self, model: Optional[str] = None, base_url: Optional[str] = None, api_key: Optional[str] = None):
        self.model = model or settings.model_planner
        self.base_url = base_url or settings.openai_base_url
        self.api_key = api_key or settings.openai_api_key
        
        self.client = OpenAI(
            base_url=self.base_url,
            api_key=self.api_key or "ollama",  # Default for Ollama
        )
        
        # Category-specific configurations
        self.category_configs = {
            "ai_papers": {
                "preferred_sites": ["arxiv.org", "paperswithcode.com", "openreview.net"],
                "freshness_days": 30,
                "include_news": False,
                "time_range": "m",
            },
            "crypto_news": {
                "preferred_sites": ["coindesk.com", "cointelegraph.com", "decrypt.co"],
                "freshness_days": 7,
                "include_news": True,
                "time_range": "w",
            },
            "tech_jobs": {
                "preferred_sites": ["ycombinator.com", "stackoverflow.com", "github.com"],
                "freshness_days": 14,
                "include_news": False,
                "time_range": "w",
            },
            "startup_news": {
                "preferred_sites": ["techcrunch.com", "venturebeat.com", "theverge.com"],
                "freshness_days": 7,
                "include_news": True,
                "time_range": "w",
            },
        }
    
    async def create_plan(
        self,
        query: str,
        max_queries: int = 6,
        max_pages: Optional[int] = None,
        custom_sites: Optional[List[str]] = None,
    ) -> CrawlPlan:
        """Create a comprehensive crawl plan for the given query"""
        
        if max_pages is None:
            max_pages = settings.max_pages_per_run
        
        logger.info(f"Creating crawl plan for query: {query}")
        
        # Generate category, queries, and planning details from the main query
        plan_details = await self._generate_initial_plan_details(query, max_queries)
        
        category = plan_details.get("category", "general")
        queries = plan_details.get("queries", [query])
        
        # Get category-specific config
        category_config = self.category_configs.get(
            category.lower().replace(" ", "_"),
            {
                "preferred_sites": [],
                "freshness_days": 7,
                "include_news": False,
                "time_range": "w",
            }
        )
        
        # Combine with category config and generated details
        preferred_sites = custom_sites or plan_details.get("preferred_sites", category_config["preferred_sites"])
        
        plan = CrawlPlan(
            category=category,
            queries=queries,
            must_have_keywords=plan_details.get("must_have_keywords", []),
            avoid_keywords=plan_details.get("avoid_keywords", []),
            preferred_sites=preferred_sites,
            avoid_domains=plan_details.get("avoid_domains", []),
            max_pages=max_pages,
            max_pages_per_domain=min(max_pages // 3, settings.max_pages_per_domain),
            freshness_days=int(category_config.get("freshness_days", 7)),
            include_news=bool(category_config.get("include_news", False)),
            time_range=str(category_config.get("time_range", "w")),
            priority_score=plan_details.get("priority_score", 0.5),
            estimated_duration_minutes=self._estimate_duration(max_pages),
        )
        
        logger.info(f"Created crawl plan for '{category}' with {len(queries)} queries for {max_pages} pages")
        return plan
    
    async def _generate_initial_plan_details(
        self,
        query: str,
        max_queries: int
    ) -> Dict[str, Any]:
        """Generate a full plan from a single complex user query."""
        
        system_prompt = """You are an expert at interpreting complex user queries and creating a detailed web search plan.
        
Your task is to analyze the user's query and extract a structured search plan.

You must return a JSON object with the following fields:
- category: A short, descriptive category for the query (e.g., "AI Research", "Market Analysis", "Software Development").
- queries: An array of {max_queries} diverse and effective search queries that break down the user's request.
- must_have_keywords: An array of keywords that are essential for relevant content.
- avoid_keywords: An array of keywords to filter out irrelevant content.
- preferred_sites: An array of high-quality websites or domains relevant to the query.
- avoid_domains: An array of low-quality domains to avoid.
- priority_score: A float from 0.0 to 1.0 indicating the query's importance.

Guidelines:
- Infer the category from the query's content.
- Generate diverse queries covering different facets of the request.
- Identify specific, high-signal keywords.
- Suggest reputable sites (e.g., academic journals, industry news, official documentation).
- Base the priority on the query's implied urgency or depth.
"""

        user_prompt = f"""User Query: "{query}"
Max Queries: {max_queries}

Analyze this query and generate a structured search plan.

Example:
User Query: "I need to find the latest research on transformer models in NLP, specifically focusing on efficiency and new architectures. I'm not interested in basic tutorials or marketing content."
JSON Output:
{{
  "category": "AI Research",
  "queries": [
    "latest transformer model architectures 2024",
    "efficient transformers NLP research papers",
    "site:arxiv.org transformer model optimization",
    "multi-head attention mechanism improvements",
    "survey of efficient transformer models"
  ],
  "must_have_keywords": ["transformer", "NLP", "efficiency", "architecture", "research"],
  "avoid_keywords": ["tutorial", "marketing", "introduction", "for beginners"],
  "preferred_sites": ["arxiv.org", "paperswithcode.com", "aclweb.org", "neurips.cc"],
  "avoid_domains": ["geeksforgeeks.org", "towardsdatascience.com"],
  "priority_score": 0.8
}}
"""

        try:
            response = await self._call_llm(system_prompt, user_prompt)
            plan_details = json.loads(response)
            
            # Ensure essential fields are present
            if "queries" not in plan_details or not plan_details["queries"]:
                plan_details["queries"] = [query]
            if "category" not in plan_details:
                plan_details["category"] = "general"
                
            return plan_details
            
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Failed to generate initial plan details: {e}")
            # Fallback to a simple plan
            return {
                "category": "general",
                "queries": [query, f"{query} latest", f"{query} overview"],
                "must_have_keywords": [],
                "avoid_keywords": ["spam", "advertisement"],
                "preferred_sites": [],
                "avoid_domains": [],
                "priority_score": 0.5,
            }
    
    async def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        """Make a call to the LLM"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.7,
                max_tokens=2000,
            )
            
            if response.choices[0].message.content is None:
                raise ValueError("LLM response content is null")
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            raise
    
    def _estimate_duration(self, max_pages: int) -> int:
        """Estimate crawl duration in minutes"""
        # Rough estimate: 2-3 seconds per page including delays
        seconds_per_page = 2.5 + settings.request_delay
        total_seconds = max_pages * seconds_per_page
        return max(1, int(total_seconds / 60))
    
    async def optimize_plan(self, plan: CrawlPlan, feedback: Dict[str, Any]) -> CrawlPlan:
        """Optimize an existing plan based on feedback"""
        
        system_prompt = """You are an expert at optimizing web crawling strategies based on performance feedback.

Your task is to analyze crawl results and feedback to improve the crawl plan for better results.

Return your response as a JSON object with suggested improvements:
- new_queries: Array of improved search queries
- adjusted_sites: Array of sites to add or prioritize
- avoid_domains: Array of domains to avoid based on poor results
- priority_adjustments: Object with priority score and reasoning"""

        feedback_summary = {
            "pages_found": feedback.get("pages_discovered", 0),
            "pages_processed": feedback.get("pages_processed", 0),
            "avg_quality_score": feedback.get("avg_score", 0),
            "top_domains": feedback.get("top_domains", []),
            "low_quality_domains": feedback.get("low_quality_domains", []),
        }

        user_prompt = f"""Current plan:
Category: {plan.category}
Queries: {plan.queries}
Preferred sites: {plan.preferred_sites}
Results: {json.dumps(feedback_summary, indent=2)}

Based on these results, suggest improvements to the crawl plan to find higher quality content more efficiently."""

        try:
            response = await self._call_llm(system_prompt, user_prompt)
            improvements = json.loads(response)
            
            # Apply improvements to create new plan
            new_plan = CrawlPlan(
                category=plan.category,
                queries=improvements.get("new_queries", plan.queries),
                must_have_keywords=plan.must_have_keywords,
                avoid_keywords=plan.avoid_keywords + improvements.get("avoid_keywords", []),
                preferred_sites=plan.preferred_sites + improvements.get("adjusted_sites", []),
                avoid_domains=plan.avoid_domains + improvements.get("avoid_domains", []),
                max_pages=plan.max_pages,
                max_pages_per_domain=plan.max_pages_per_domain,
                freshness_days=plan.freshness_days,
                include_news=plan.include_news,
                time_range=plan.time_range,
                priority_score=improvements.get("priority_adjustments", {}).get("score", plan.priority_score),
                estimated_duration_minutes=plan.estimated_duration_minutes,
            )
            
            logger.info(f"Optimized plan for category: {plan.category}")
            return new_plan
            
        except Exception as e:
            logger.error(f"Failed to optimize plan: {e}")
            return plan


# Default planner instance
default_planner = LLMPlanner()