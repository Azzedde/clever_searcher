"""LLM-based planning component for query generation and crawl strategy"""

import json
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta

from openai import OpenAI

from ..utils.config import settings

logger = logging.getLogger(__name__)


@dataclass
class CrawlPlan:
    """Represents a crawl plan generated by the LLM"""
    category: str
    queries: List[str]
    must_have_keywords: List[str]
    avoid_keywords: List[str]
    preferred_sites: List[str]
    avoid_domains: List[str]
    max_pages: int
    max_pages_per_domain: int
    freshness_days: int
    include_news: bool
    time_range: Optional[str]  # 'd', 'w', 'm', 'y'
    priority_score: float
    estimated_duration_minutes: int
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "category": self.category,
            "queries": self.queries,
            "must_have_keywords": self.must_have_keywords,
            "avoid_keywords": self.avoid_keywords,
            "preferred_sites": self.preferred_sites,
            "avoid_domains": self.avoid_domains,
            "max_pages": self.max_pages,
            "max_pages_per_domain": self.max_pages_per_domain,
            "freshness_days": self.freshness_days,
            "include_news": self.include_news,
            "time_range": self.time_range,
            "priority_score": self.priority_score,
            "estimated_duration_minutes": self.estimated_duration_minutes,
        }


class LLMPlanner:
    """LLM-powered planning for web discovery"""
    
    def __init__(self, model: str = None, base_url: str = None, api_key: str = None):
        self.model = model or settings.model_planner
        self.base_url = base_url or settings.openai_base_url
        self.api_key = api_key or settings.openai_api_key
        
        self.client = OpenAI(
            base_url=self.base_url,
            api_key=self.api_key or "ollama",  # Default for Ollama
        )
        
        # Category-specific configurations
        self.category_configs = {
            "ai_papers": {
                "preferred_sites": ["arxiv.org", "paperswithcode.com", "openreview.net"],
                "freshness_days": 30,
                "include_news": False,
                "time_range": "m",
            },
            "crypto_news": {
                "preferred_sites": ["coindesk.com", "cointelegraph.com", "decrypt.co"],
                "freshness_days": 7,
                "include_news": True,
                "time_range": "w",
            },
            "tech_jobs": {
                "preferred_sites": ["ycombinator.com", "stackoverflow.com", "github.com"],
                "freshness_days": 14,
                "include_news": False,
                "time_range": "w",
            },
            "startup_news": {
                "preferred_sites": ["techcrunch.com", "venturebeat.com", "theverge.com"],
                "freshness_days": 7,
                "include_news": True,
                "time_range": "w",
            },
        }
    
    async def create_plan(
        self,
        category: str,
        user_query: str = "",
        max_queries: int = 6,
        max_pages: int = None,
        custom_sites: List[str] = None,
    ) -> CrawlPlan:
        """Create a comprehensive crawl plan for the given category"""
        
        if max_pages is None:
            max_pages = settings.max_pages_per_run
        
        logger.info(f"Creating crawl plan for category: {category}")
        
        # Get category-specific config
        category_config = self.category_configs.get(
            category.lower().replace(" ", "_"), 
            {
                "preferred_sites": [],
                "freshness_days": 7,
                "include_news": False,
                "time_range": "w",
            }
        )
        
        # Generate queries using LLM
        queries = await self._generate_queries(category, user_query, max_queries)
        
        # Generate additional planning details
        planning_details = await self._generate_planning_details(category, queries)
        
        # Combine with category config
        preferred_sites = custom_sites or category_config["preferred_sites"]
        
        plan = CrawlPlan(
            category=category,
            queries=queries,
            must_have_keywords=planning_details.get("must_have_keywords", []),
            avoid_keywords=planning_details.get("avoid_keywords", []),
            preferred_sites=preferred_sites,
            avoid_domains=planning_details.get("avoid_domains", []),
            max_pages=max_pages,
            max_pages_per_domain=min(max_pages // 3, settings.max_pages_per_domain),
            freshness_days=category_config["freshness_days"],
            include_news=category_config["include_news"],
            time_range=category_config["time_range"],
            priority_score=planning_details.get("priority_score", 0.5),
            estimated_duration_minutes=self._estimate_duration(max_pages),
        )
        
        logger.info(f"Created crawl plan with {len(queries)} queries for {max_pages} pages")
        return plan
    
    async def _generate_queries(
        self, 
        category: str, 
        user_query: str, 
        max_queries: int
    ) -> List[str]:
        """Generate diverse search queries for the category"""
        
        system_prompt = """You are an expert at generating diverse, effective search queries for web discovery.
        
Your task is to create search queries that will find the most relevant and high-quality content for a given category.

Guidelines:
- Generate diverse queries that cover different aspects of the topic
- Use specific terminology and keywords relevant to the domain
- Include both broad and specific queries
- Consider using site: operators for known high-quality sources
- Avoid overly generic queries
- Make queries that would work well with search engines like DuckDuckGo

Return your response as a JSON object with a "queries" field containing an array of strings."""

        user_prompt = f"""Category: {category}
User query: {user_query}
Max queries: {max_queries}

Generate {max_queries} diverse search queries for this category. If a user query is provided, incorporate it into the queries while still maintaining diversity.

Examples for different categories:
- AI Papers: "transformer architecture papers", "site:arxiv.org attention mechanism", "neural network optimization 2024"
- Crypto News: "bitcoin price analysis", "ethereum upgrade news", "cryptocurrency regulation"
- Tech Jobs: "python developer remote", "site:ycombinator.com hiring", "machine learning engineer jobs"

Focus on creating queries that will find high-quality, recent content relevant to the category."""

        try:
            response = await self._call_llm(system_prompt, user_prompt)
            queries_data = json.loads(response)
            queries = queries_data.get("queries", [])
            
            # Ensure we have at least one query
            if not queries:
                queries = [category]
            
            return queries[:max_queries]
            
        except Exception as e:
            logger.error(f"Failed to generate queries: {e}")
            # Fallback to simple queries
            return [category, f"{category} latest", f"{category} news"]
    
    async def _generate_planning_details(
        self, 
        category: str, 
        queries: List[str]
    ) -> Dict[str, Any]:
        """Generate additional planning details like keywords and priorities"""
        
        system_prompt = """You are an expert content curator who helps filter and prioritize web content.

Your task is to analyze a category and its search queries to provide filtering criteria and priorities.

Return your response as a JSON object with these fields:
- must_have_keywords: Array of keywords that high-quality content should contain
- avoid_keywords: Array of keywords that indicate low-quality or irrelevant content
- avoid_domains: Array of domains known for low-quality content (spam, clickbait, etc.)
- priority_score: Float between 0-1 indicating how important/urgent this category is
- content_quality_indicators: Array of phrases that indicate high-quality content"""

        user_prompt = f"""Category: {category}
Search queries: {', '.join(queries)}

Analyze this category and provide filtering criteria to help identify high-quality, relevant content while avoiding spam and low-quality sources.

Consider:
- What keywords indicate authoritative, well-researched content?
- What keywords suggest clickbait, spam, or low-quality content?
- What domains are known for poor content quality in this domain?
- How urgent/important is this type of content (0.1 = low priority, 0.9 = high priority)?"""

        try:
            response = await self._call_llm(system_prompt, user_prompt)
            # Clean up the response to handle potential JSON issues
            cleaned_response = response.strip()
            if not cleaned_response.startswith('{'):
                # Find the first { and last }
                start = cleaned_response.find('{')
                end = cleaned_response.rfind('}')
                if start != -1 and end != -1:
                    cleaned_response = cleaned_response[start:end+1]
            
            return json.loads(cleaned_response)
            
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Failed to generate planning details: {e}")
            logger.debug(f"Raw response: {response[:500] if 'response' in locals() else 'No response'}")
            return {
                "must_have_keywords": [],
                "avoid_keywords": ["clickbait", "spam", "advertisement"],
                "avoid_domains": ["example.com"],
                "priority_score": 0.5,
                "content_quality_indicators": [],
            }
    
    async def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        """Make a call to the LLM"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.7,
                max_tokens=2000,
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            raise
    
    def _estimate_duration(self, max_pages: int) -> int:
        """Estimate crawl duration in minutes"""
        # Rough estimate: 2-3 seconds per page including delays
        seconds_per_page = 2.5 + settings.request_delay
        total_seconds = max_pages * seconds_per_page
        return max(1, int(total_seconds / 60))
    
    async def optimize_plan(self, plan: CrawlPlan, feedback: Dict[str, Any]) -> CrawlPlan:
        """Optimize an existing plan based on feedback"""
        
        system_prompt = """You are an expert at optimizing web crawling strategies based on performance feedback.

Your task is to analyze crawl results and feedback to improve the crawl plan for better results.

Return your response as a JSON object with suggested improvements:
- new_queries: Array of improved search queries
- adjusted_sites: Array of sites to add or prioritize
- avoid_domains: Array of domains to avoid based on poor results
- priority_adjustments: Object with priority score and reasoning"""

        feedback_summary = {
            "pages_found": feedback.get("pages_discovered", 0),
            "pages_processed": feedback.get("pages_processed", 0),
            "avg_quality_score": feedback.get("avg_score", 0),
            "top_domains": feedback.get("top_domains", []),
            "low_quality_domains": feedback.get("low_quality_domains", []),
        }

        user_prompt = f"""Current plan:
Category: {plan.category}
Queries: {plan.queries}
Preferred sites: {plan.preferred_sites}
Results: {json.dumps(feedback_summary, indent=2)}

Based on these results, suggest improvements to the crawl plan to find higher quality content more efficiently."""

        try:
            response = await self._call_llm(system_prompt, user_prompt)
            improvements = json.loads(response)
            
            # Apply improvements to create new plan
            new_plan = CrawlPlan(
                category=plan.category,
                queries=improvements.get("new_queries", plan.queries),
                must_have_keywords=plan.must_have_keywords,
                avoid_keywords=plan.avoid_keywords + improvements.get("avoid_keywords", []),
                preferred_sites=plan.preferred_sites + improvements.get("adjusted_sites", []),
                avoid_domains=plan.avoid_domains + improvements.get("avoid_domains", []),
                max_pages=plan.max_pages,
                max_pages_per_domain=plan.max_pages_per_domain,
                freshness_days=plan.freshness_days,
                include_news=plan.include_news,
                time_range=plan.time_range,
                priority_score=improvements.get("priority_adjustments", {}).get("score", plan.priority_score),
                estimated_duration_minutes=plan.estimated_duration_minutes,
            )
            
            logger.info(f"Optimized plan for category: {plan.category}")
            return new_plan
            
        except Exception as e:
            logger.error(f"Failed to optimize plan: {e}")
            return plan


# Default planner instance
default_planner = LLMPlanner()